{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c0cab8",
   "metadata": {
    "id": "67c0cab8"
   },
   "source": [
    "# Tarea Final NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e70bb7",
   "metadata": {
    "id": "c9e70bb7"
   },
   "source": [
    "Autor: Salvador Yábar Reaño\n",
    "\n",
    "Estudio exploratorio de 3 modelos de clasificación de emociones. Dataset: https://github.com/fmplaza/EmoEvent/tree/master/splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f6bd6c",
   "metadata": {
    "id": "82f6bd6c"
   },
   "source": [
    "### Importar dataset y librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ddfc10",
   "metadata": {
    "id": "80ddfc10"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81005da5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "81005da5",
    "outputId": "ea3f2376-9c19-4081-a3bb-9097fde30e31"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"emoevent_es.csv\", sep=None, engine='python', index_col=0) # Cargar dataset\n",
    "df = df.drop(columns=[\"offensive\"]) # Eliminar columna no necesaria\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03345e1e",
   "metadata": {
    "id": "03345e1e"
   },
   "source": [
    "Se observa la estructura del dataset. Se tienen las columnas 'tweet' y 'emotion'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81961a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c81961a0",
    "outputId": "7c5194c4-9b49-4445-88e4-cfa52aa2bfd2"
   },
   "outputs": [],
   "source": [
    "df.info() # Revisar cantidad de tweets y tipos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ef19e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "6b6ef19e",
    "outputId": "87246fbc-5ced-4cbe-c1f7-71b57ba564fd"
   },
   "outputs": [],
   "source": [
    "df[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e6687",
   "metadata": {
    "id": "734e6687"
   },
   "source": [
    "Emotion cuenta con 7 categorías: others, joy, sadness, anger, surprise, disgust y fear.\n",
    "\n",
    "Se observa una clara desproporción entre las clases, siendo la que menos ejemplos tiene la clase fear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608218a4",
   "metadata": {
    "id": "608218a4"
   },
   "source": [
    "# Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4078d71",
   "metadata": {
    "id": "c4078d71"
   },
   "source": [
    "Para el desarrollo de este estudio exploratorio se seleccionaron 3 enfoques diferentes, cada uno empleando una aproximación distinta al problema de clasificación.\n",
    "\n",
    "Se presentará el desarrollo de cada modelo, sus resultados y observaciones, y finalmente se compararán los resultados obtenidos entre los tres modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a41f4",
   "metadata": {
    "id": "ed5a41f4"
   },
   "source": [
    "## Modelo 1: TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb71df3",
   "metadata": {
    "id": "2cb71df3"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d15e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "844d15e2",
    "outputId": "e717cb00-6e4d-4068-a961-70764e857a21"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "\n",
    "tokenizer = ToktokTokenizer() # Inicializar tokenizador\n",
    "stop_words = set(stopwords.words('spanish')) # stopwords en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7cc426",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d7cc426",
    "outputId": "4abd16be-4eaf-45d6-a91c-f875479f525d"
   },
   "outputs": [],
   "source": [
    "def remove_tweet_chars(text): # Eliminar caracteres innecesarios en los tweets\n",
    "  text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','', str(text))\n",
    "  text = re.sub('@[^\\s]+','',text)\n",
    "  return text\n",
    "\n",
    "def remove_words(tokens):\n",
    "    return [tok for tok in tokens if tok not in stop_words and not tok.isnumeric() and re.search(r'\\w', tok)]\n",
    "\n",
    "def preprocess_tweets_es(dataset, text_col=\"tweet\"): # Procesar los tweets, aplicando las funciones anteriores y tokenizando\n",
    "    dataset = dataset.copy()\n",
    "    dataset[\"CLEAN\"] = dataset[text_col].apply(remove_tweet_chars)\n",
    "    dataset[\"CLEAN\"] = dataset[\"CLEAN\"].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "    dataset[\"CLEAN\"] = dataset[\"CLEAN\"].apply(remove_words)\n",
    "    dataset[\"CLEAN_STR\"] = dataset[\"CLEAN\"].apply(lambda toks: \" \".join(toks)) # Crear columna con tweets limpios como string\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4802f5",
   "metadata": {
    "id": "1c4802f5"
   },
   "source": [
    "Aplicando las funciones anteriores, se obtiene un dataset con columnas CLEAN y CLEAN_STR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20476782",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "20476782",
    "outputId": "0d033ebf-2b45-477d-ddef-fd222469bb3a"
   },
   "outputs": [],
   "source": [
    "df_clean = preprocess_tweets_es(df, text_col=\"tweet\")\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941f896",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7941f896",
    "outputId": "ccdb588e-29ec-4a60-df66-2cd291c2ff42"
   },
   "outputs": [],
   "source": [
    "df_clean['CLEAN'][1] # Tweet limpio como lista de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f628b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "84f628b9",
    "outputId": "593915db-1f07-484f-f6a3-6c861e1cd164"
   },
   "outputs": [],
   "source": [
    "df_clean['CLEAN_STR'][1] # Tweet limpio como string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xq0uZ_bJLydK",
   "metadata": {
    "id": "xq0uZ_bJLydK"
   },
   "source": [
    "### Funciones para resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TSBXBxJkLyHj",
   "metadata": {
    "id": "TSBXBxJkLyHj"
   },
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# DF de resultados\n",
    "def get_f1_per_class(y_true, y_pred, labels, model_name):\n",
    "    report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=labels,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    # Extraemos solo el f1-score por clase\n",
    "    f1_scores = {cls: report[cls]['f1-score'] for cls in labels}\n",
    "    return pd.DataFrame(f1_scores, index=[model_name])\n",
    "\n",
    "df_results = pd.DataFrame() # Dataframe de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ffaed",
   "metadata": {
    "id": "4a7ffaed"
   },
   "source": [
    "Las columans obtenidas servirán tanto para el clasificador con Logistic Regression como con LSTM, ya que la segunda necesita el string completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea7778",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "bdea7778",
    "outputId": "0ac5b7a0-2514-48b0-b132-117ca32d319a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Separar en conjunto de entrenamiento y prueba\n",
    "X = df_clean[\"CLEAN_STR\"]\n",
    "y = df_clean[\"emotion\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Vectorización TF-IDF\n",
    "vectorizer = TfidfVectorizer(lowercase=False)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf  = vectorizer.transform(X_test)\n",
    "\n",
    "# Regresión logística (clasificador)\n",
    "clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluación\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "plot_confusion_matrix(y_test, y_pred, classes=clf.classes_)\n",
    "f1_model1 = get_f1_per_class(y_test, y_pred, clf.classes_, \"TF-IDF + LR\")\n",
    "df_results = pd.concat([df_results, f1_model1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b582e74",
   "metadata": {
    "id": "1b582e74"
   },
   "source": [
    "### Resultados modelo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548643d",
   "metadata": {
    "id": "0548643d"
   },
   "source": [
    "Se observa que el modelo tuvo una precisión y recall muy bajos en la clase disgust. El disgusto puede estar asociado con el enojo, clase que predijo el modelo para estos ejemplos. Esto se relaciona con el número reducido de ejemplos de disgust, por lo que el modelo no consigue una buena generalización a partir de estos.\n",
    "\n",
    "Por otro lado, se observa que gran parte de las predicciones fueron para la clase others. Al ser una clase que agrupa tweets variados, es comprensible que al no tener certeza de otras clases, el modelo prediga esta. Esto se refleja en el alto recall con una precisión media, recupera gran parte de los ejemplos en others, pero también clasifica ejemplos de otras clases en esta.\n",
    "\n",
    "En general, los mejores resultados se dieron para joy, others y sadness. Las demás categorías tienen un F1-Score muy bajo. Esto se puede deber en gran medida a que existen más ejemplos para estas clases, así como que felicidad y tristeza son emociones que son más faciles de denotar.\n",
    "\n",
    "El uso de métodos clásicos para el preprocesamiento (quitar caracteres y stopwords, tokenizar) es necesario para este modelo de clasificación, y sin embargo no se consiguen resultados destacables, lo que es esperable. De esta forma se demuestra la limitación de este método, ya que carece de la interpretación contextual, por lo que emociones más complejas no son predichas correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b89e15",
   "metadata": {
    "id": "d0b89e15"
   },
   "source": [
    "## Modelo 2: Embeddings + Red Recurrente (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677807c8",
   "metadata": {
    "id": "677807c8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f97971",
   "metadata": {
    "id": "c5f97971"
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 10000   # Tamaño del vocabulario\n",
    "MAX_SEQ_LEN   = 48\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\", filters='')\n",
    "texts = df_clean[\"CLEAN_STR\"].astype(str).tolist()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_int = le.fit_transform(df_clean[\"emotion\"])\n",
    "y = tf.keras.utils.to_categorical(y_int, num_classes=len(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3e6a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "id": "6ae3e6a6",
    "outputId": "6974b68e-66d5-471a-c534-3e72fbb4fcff"
   },
   "outputs": [],
   "source": [
    "# Definir el modelo de red neuronal\n",
    "EMBEDDING_DIM = 32\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(MAX_SEQ_LEN,)),\n",
    "    Embedding(input_dim=MAX_NUM_WORDS, output_dim=EMBEDDING_DIM, mask_zero=True),\n",
    "    LSTM(16, dropout=0.6, recurrent_dropout=0.2, return_sequences=False),\n",
    "    Dropout(0.6),\n",
    "    Dense(len(le.classes_), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=Adam(learning_rate=5e-4),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e8897d",
   "metadata": {
    "id": "17e8897d"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y_int\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42, stratify=np.argmax(y_train, axis=1)\n",
    ")\n",
    "\n",
    "# calcular class_weight con etiquetas enteras del train para balancear clases\n",
    "y_train_int = np.argmax(y_train, axis=1)\n",
    "classes = np.unique(y_train_int)\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train_int)\n",
    "class_weight = {i: w for i, w in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "msEAp2LkYGMX",
   "metadata": {
    "id": "msEAp2LkYGMX"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_DIR  = Path(\"models/lstm_emotions\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PATH = MODEL_DIR / \"model.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QgfilC2kYcBn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgfilC2kYcBn",
    "outputId": "5eaac803-5eae-4981-e307-898c6d3e11e1"
   },
   "outputs": [],
   "source": [
    "if MODEL_PATH.exists():\n",
    "    print(f\"Existe modelo guardado previamente: {MODEL_PATH}. Saltando el entrenamiento.\")\n",
    "    model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "else:\n",
    "\n",
    "  history = model.fit(\n",
    "      X_train, y_train,\n",
    "      validation_data=(X_val, y_val),\n",
    "      batch_size=64,\n",
    "      epochs=25,\n",
    "      verbose=1,\n",
    "      callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3,\n",
    "                                          restore_best_weights=True, verbose=1),\n",
    "                tf.keras.callbacks.ModelCheckpoint(filepath=str(MODEL_PATH), monitor='val_loss',\n",
    "                                                    save_best_only=True, save_weights_only=False, verbose=1)],\n",
    "      class_weight=class_weight\n",
    "  )\n",
    "  print(f\"Guardando el modelo en: {MODEL_PATH}\")\n",
    "\n",
    "model.load_weights(\"models/lstm_emotions/model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591168e",
   "metadata": {
    "id": "c591168e"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_true_str = le.inverse_transform(y_true)\n",
    "y_pred_str = le.inverse_transform(y_pred_classes)\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes, target_names=le.classes_, zero_division=0))\n",
    "f1_model2 = get_f1_per_class(y_true_str, y_pred_str, le.classes_, \"LSTM\")\n",
    "df_results = pd.concat([df_results, f1_model2])\n",
    "plot_confusion_matrix(y_true_str, y_pred_str, classes=le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uB-97Uy58pYa",
   "metadata": {
    "id": "uB-97Uy58pYa"
   },
   "source": [
    "### Resultados modelo 2:\n",
    "Durante el entrenamiento, se observó que la red LSTM es propensa a presentar overfitting al conjunto de datos, por lo que se tuvo que aplicar técnicas de regularización (dropout) y disminuir algunos parámetros como la cantidad de neuronas para simplificar el modelo.\n",
    "\n",
    "Tras varias iteraciones, se logró reducir el overfitting.\n",
    "\n",
    "Si bien se obtuvo un menor desempeño en las clases que el modelo 1 predecía correctamente (joy, others, sadness), se obtuvieron mejores resultados para las demás clases.\n",
    "\n",
    "Como se observa en la matriz de confusión, se tienen resultados más balanceados de precisión y recall para todas las clases.\n",
    "\n",
    "Como en el modelo anterior, las clases de others y sadness destacan por tener un mejor desempeño que las demás.  \n",
    "\n",
    "Este modelo puede interpretar el contexto de la oración gracias a emplear LSTM, de esta forma, se logra interpretar emociones más complejas que pueden ser expresadas de manera más sutil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad43208b",
   "metadata": {
    "id": "ad43208b"
   },
   "source": [
    "## Modelo 3: Fine-tune a BETO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_1dJ8AaWDN8g",
   "metadata": {
    "id": "_1dJ8AaWDN8g"
   },
   "source": [
    "El tercer modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2d110",
   "metadata": {
    "id": "a1a2d110"
   },
   "outputs": [],
   "source": [
    "df_bert = df.copy()\n",
    "df_bert[\"text\"] = df_bert[\"tweet\"].apply(remove_tweet_chars)\n",
    "df_bert = df_bert[df_bert[\"text\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_bert[\"label\"] = le.fit_transform(df_bert[\"emotion\"])\n",
    "num_labels = len(le.classes_)  # should be 7\n",
    "\n",
    "X = df_bert[\"text\"].values\n",
    "y = df_bert[\"label\"].values\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.5, random_state=42, stratify=y_tmp\n",
    ")\n",
    "\n",
    "print(\"Sizes:\", len(X_train), len(X_val), len(X_test))\n",
    "print(\"Classes:\", list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b90106",
   "metadata": {
    "id": "d0b90106"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "MODEL_NAME = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "SAVE_DIR   = Path(\"beto-emotions-tf\")\n",
    "MAX_LEN = 96\n",
    "BATCH   = 16\n",
    "NUM_LABELS = len(le.classes_)\n",
    "\n",
    "# Cargar si existe tokenizer guardado\n",
    "if SAVE_DIR.exists():\n",
    "    print(\"Loading tokenizer from saved folder…\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR)\n",
    "else:\n",
    "    print(\"Downloading tokenizer from Hugging Face…\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "# ---------- Encode datasets ----------\n",
    "enc_train = tokenizer(list(X_train), truncation=True, padding=\"max_length\",\n",
    "                      max_length=MAX_LEN, return_tensors=\"np\")\n",
    "enc_val   = tokenizer(list(X_val),   truncation=True, padding=\"max_length\",\n",
    "                      max_length=MAX_LEN, return_tensors=\"np\")\n",
    "enc_test  = tokenizer(list(X_test),  truncation=True, padding=\"max_length\",\n",
    "                      max_length=MAX_LEN, return_tensors=\"np\")\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((\n",
    "    {\"input_ids\": enc_train[\"input_ids\"], \"attention_mask\": enc_train[\"attention_mask\"]},\n",
    "    y_train\n",
    ")).shuffle(2048, seed=42).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((\n",
    "    {\"input_ids\": enc_val[\"input_ids\"], \"attention_mask\": enc_val[\"attention_mask\"]},\n",
    "    y_val\n",
    ")).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((\n",
    "    {\"input_ids\": enc_test[\"input_ids\"], \"attention_mask\": enc_test[\"attention_mask\"]},\n",
    "    y_test\n",
    ")).batch(BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3a72fa",
   "metadata": {
    "id": "ae3a72fa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_KERAS\"] = \"1\"\n",
    "\n",
    "import tf_keras as keras\n",
    "from tf_keras.optimizers import Adam\n",
    "from tf_keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "# Saltar el entrenamiento si existe un modelo entrenado localmente\n",
    "if SAVE_DIR.exists():\n",
    "    print(\"Found saved model, loading instead of training.\")\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(SAVE_DIR)\n",
    "    history = None\n",
    "else:\n",
    "    print(\"Training new model…\")\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=NUM_LABELS\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=2e-5),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n",
    "    )\n",
    "    cbs = [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=1, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, min_lr=1e-6),\n",
    "    ]\n",
    "\n",
    "    classes = np.unique(y_train)\n",
    "    weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "    class_weight = {int(c): float(w) for c, w in zip(classes, weights)}\n",
    "\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=3, callbacks=cbs, verbose=1, class_weight=class_weight)\n",
    "    model.save_pretrained(SAVE_DIR)\n",
    "    tokenizer.save_pretrained(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZXaD9bJsoDTV",
   "metadata": {
    "id": "ZXaD9bJsoDTV"
   },
   "outputs": [],
   "source": [
    "# 1) Get logits\n",
    "all_logits = []\n",
    "for inputs, _ in test_ds:\n",
    "    out = model(inputs, training=False)\n",
    "    all_logits.append(out.logits.numpy())\n",
    "logits = np.concatenate(all_logits, axis=0)\n",
    "\n",
    "# 2) Integers → strings\n",
    "y_pred = np.argmax(logits, axis=1)\n",
    "y_true = y_test\n",
    "\n",
    "y_true_str = le.inverse_transform(y_true)\n",
    "y_pred_str = le.inverse_transform(y_pred)\n",
    "\n",
    "f1_model3 = get_f1_per_class(y_true_str, y_pred_str, le.classes_, \"BETO\")\n",
    "df_results = pd.concat([df_results, f1_model3])\n",
    "\n",
    "# Resultados\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_, zero_division=0))\n",
    "plot_confusion_matrix(y_true_str, y_pred_str, classes=le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UOzyECzlFSxw",
   "metadata": {
    "id": "UOzyECzlFSxw"
   },
   "source": [
    "### Resultados Modelo 3:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C2t0TKy4Uq3m",
   "metadata": {
    "id": "C2t0TKy4Uq3m"
   },
   "source": [
    "Como se observó en los otros modelos, las clases de joy, others, y sadness obtuvieron el mejor desempeño con F1-Scores mayores a 0.6.\n",
    "\n",
    "Algo a resaltar en este modelo es que una gran cantidad de ejemplos pertenecientes a anger fueron clasificados como disgust por el modelo. De esta forma, a pesar de tener un recall del 100%, la precisión en esta clase es considerablemente baja (13%). Esto sugiere que el modelo tiene dificultad en diferenciar emociones negativas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IZZNKzGgPZ8i",
   "metadata": {
    "id": "IZZNKzGgPZ8i"
   },
   "source": [
    "\n",
    "# Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2rx5XSeSDNi",
   "metadata": {
    "id": "p2rx5XSeSDNi"
   },
   "source": [
    "Tabla de F1-Scores para los 3 modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VZkm-2pEPbOT",
   "metadata": {
    "id": "VZkm-2pEPbOT"
   },
   "outputs": [],
   "source": [
    "print(df_results.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QsiBNrTwPgPD",
   "metadata": {
    "id": "QsiBNrTwPgPD"
   },
   "source": [
    "A partir de la tabla, se puede observar mejor cómo los 3 modelos se comparan entre sí.\n",
    "\n",
    "En primer lugar, BETO consigue el mayor F1-Score en la mayoría de categorías, seguido del modelo 1, que consigue el mejor desempeño en anger y fear. El modelo 2 no llega a superar a los otros dos en ninguna clase.\n",
    "\n",
    "Lo anterior puede deberse a varios factores. Las clases de anger y fear pueden estar representadas por palabras clave específicos, que el modelo 1 basado en frecuencia clasifica correctamente en estas clases.\n",
    "\n",
    "Por otro lado, al tener un dataset relativamente pequeño, un modelo de Deep Learning como LSTM tiende a experimentar overfitting y no aprender buenas generalizaciones.\n",
    "\n",
    "BETO destaca por su capacidad para capturar relaciones más complejas entre palabras, lo que se refleja en su mejor rendimiento. No obstante, esto tiene un costo computacional importante, su entrenamiento requiere considerablemente más tiempo y recursos que los otros modelos. A pesar de ello, su mejora sobre el modelo de Logistic Regression es clara, aunque no drástica.\n",
    "\n",
    "En conclusión, para este problema y bajo las limitaciones del dataset, BETO es la opción más robusta y precisa. Sin embargo, si se buscan modelos más livianos, enfoques clásicos como TF-IDF + Logistic Regression aún ofrecen resultados razonablemente competitivos."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
